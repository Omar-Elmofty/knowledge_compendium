{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "853e4258-c47d-4ff6-9f38-57a58e0f436b",
   "metadata": {},
   "source": [
    "# Markov Decision Processes\n",
    "\n",
    "Notes for the following lecture:    \n",
    "[Lecture Notes](https://www.youtube.com/watch?v=lfHX2hHRMVQ)     \n",
    "[Lecture Slides](https://davidstarsilver.wordpress.com/wp-content/uploads/2025/04/lecture-2-mdp.pdf)\n",
    "\n",
    "\n",
    "## What is an MDP?\n",
    "\n",
    "A markhov desicion process is a process that describes an environment in which an agent can act. An MDP is **fully observable** (meaning that the agent knows everything about the state of the environment).\n",
    "\n",
    "Almost all RL problems can be converted into some form of MDP (even partially observable ones).\n",
    "\n",
    "\n",
    "## The Markov Property\n",
    "\n",
    "As described in the previous lecture, the markov property of the state where:\n",
    "\n",
    "$$\n",
    "P[S_{t+1}|S_t] = P[S_{t+1}|S_1, ..., S_t]\n",
    "$$\n",
    "\n",
    "Meaning that the future is only conditioned on the present not the past. The future only depends on the present and is independent of the past. The present state contains all the relevent information to determine the future. Once the present state is known the entire history can be thrown away (i.e. the current state is a sufficient statistic of the future).\n",
    "\n",
    "## The Markov Process\n",
    "\n",
    "A Markov process is a random process with some transition dynamics. A Markov process involves state transitions from one state ot the other. $S_1 \\rightarrow S_2 \\rightarrow ... S_n$\n",
    "\n",
    "The definition of the Markhov process involes defining:\n",
    "1. $S$ is a finite set of states\n",
    "2. $P$ is a state transition probability matrix: $P_{ss'} = P[S_{t+1} = s' | S_t = s]$\n",
    "\n",
    "\n",
    "Below is an example of a markov process. Our state is the location of the agent on the grid (x, y indices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "377b82f2-e409-4e6d-8c7f-1893cd99860c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2]\n",
      "[4 2]\n",
      "[4 3]\n",
      "[4 4]\n"
     ]
    }
   ],
   "source": [
    "# Let's create an example of a markov\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "N = 10 # grid size nxn\n",
    "\n",
    "actions = [ np.array([1, 0]), # right\n",
    "            np.array([0, 1]), # up\n",
    "            np.array([-1, 0]), # left\n",
    "            np.array([0, -1])] # down\n",
    "\n",
    "# Random policy that equally balances between actions\n",
    "action_probabilities = [0.25, 0.25, 0.25, 0.25]\n",
    "\n",
    "\n",
    "def markov_process(s, N, actions, action_probabilities):\n",
    "    index = np.random.choice(len(actions), p=action_probabilities)\n",
    "    action = actions[index]\n",
    "    return np.clip(s + action, 0, N -1)\n",
    "    \n",
    "\n",
    "# Initialize state randomly\n",
    "state = np.array([random.randint(0, N - 1), random.randint(0, N - 1)])\n",
    "print(state)\n",
    "for _ in range(3):\n",
    "    # Transtion from state s to state s'\n",
    "    state = markov_process(state, N, actions, action_probabilities)\n",
    "    print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95b57d4-d7b6-41f9-ac05-8980b9b59b8b",
   "metadata": {},
   "source": [
    "## Markov Reward Process\n",
    "\n",
    "Similar to the Markov process, but with the addition of rewards. The markov reward process involves the following:\n",
    "1. $S$ is a finite set of states\n",
    "2. $P$ is a state transition probability matrix: $P_{ss'} = P[S_{t+1} = s' | S_t = s]$\n",
    "3. $R$ is a reward function where $R_s = E[R_{t+1} | S_t = s]$\n",
    "4. $\\gamma$ is a discount factor, $\\gamma \\in [0,1]$\n",
    "\n",
    "\n",
    "One could ask what's the need for the discount factor:\n",
    "1. Makes mathematics easy (reward doesn't explode to infinity for non-terminating or cyclical MDPs\n",
    "2. Discount factor represents uncertainty in future rewards, this mimics natural human behavior\n",
    "3. If the sequences are guaranteed to terminate then you could use a discount factor equal to 1\n",
    "\n",
    "Below is an example of a markhov reward process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c33a9e2-0d34-4997-a4e8-be1fec377417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 6]\n",
      "[9 5] reward for just taking this action -1\n",
      "[9 4] reward for just taking this action -1\n",
      "[9 4] reward for just taking this action -1\n"
     ]
    }
   ],
   "source": [
    "rewards = [-1, # reward for moving right\n",
    "           -1, # reward for moving up\n",
    "           -1, # reward for moving left\n",
    "           -1] # reward for moving down\n",
    "\n",
    "def markov_reward_process(s, N, actions, action_probabilities, rewards):\n",
    "    index = np.random.choice(len(actions), p=action_probabilities)\n",
    "    action = actions[index]\n",
    "    reward = rewards[index]\n",
    "    return np.clip(s + action, 0, N -1), reward\n",
    "\n",
    "\n",
    "# Initialize state randomly\n",
    "state = np.array([random.randint(0, N - 1), random.randint(0, N - 1)])\n",
    "print(state)\n",
    "for _ in range(3):\n",
    "    # Transtion from state s to state s'\n",
    "    state, reward = markov_reward_process(state, N, actions, action_probabilities, rewards)\n",
    "    print(state, \"reward for just taking this action\",reward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e9e2109-49e6-4787-a74c-fc05803a8db0",
   "metadata": {},
   "source": [
    "Now let's define the return or the value\n",
    "\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + ... = \\Sigma_{k=0}^{\\inf} \\gamma^k R_{t+k+1}\n",
    "$$\n",
    "\n",
    "if the value of $\\gamma$ is close to zero this leads to \"myopic\" evaluation    \n",
    "if the valye of $\\gamma$ is close to 1 this leads to \"far-sighted\" evaluation\n",
    "\n",
    "We can now define the value function,\n",
    "\n",
    "## Value Function\n",
    "The value function is defined as\n",
    "\n",
    "$$\n",
    "v(s) = E[G_t|S_t = s]\n",
    "$$\n",
    "\n",
    "The above is called the state value function.\n",
    "\n",
    "The value function represents the goodness of being at a specific state, it means the sum of discounted rewards that could be achieved at that state.\n",
    "\n",
    "A recursive relationship can be defined using the value function forming the bellman equation\n",
    "\n",
    "## Bellman Equation\n",
    "The bellman equation defines a recursive relationship between the value function and itself.\n",
    "\n",
    "$$\n",
    "v(s) = E[R_{t+1} + \\gamma v(S_{t+1}) | S_{t} = s]\n",
    "$$\n",
    "\n",
    "You could think of it using the following backup diagram\n",
    "\n",
    "\n",
    "![mdp_branching](../../images/mdp_branching.png)\n",
    "\n",
    "The Bellman equation can also be defined in matrix form:\n",
    "\n",
    "$$\n",
    "v = R + \\gamma P v\n",
    "$$\n",
    "\n",
    "Where $P$ is the transition probability\n",
    "\n",
    "You could then solve the bellman equation:\n",
    "\n",
    "$$\n",
    "v = (1 - \\gamma P)^{-1} R\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94131843-0861-4f45-a88d-41ba814515b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-19.99999999],\n",
       "       [-10.        ],\n",
       "       [  0.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's attempt solving for the value function in matrix form\n",
    "# Consider the below example where you have 3 states\n",
    "#  S1 -> S2 -> S3\n",
    "\n",
    "\n",
    "# Let's build the state vector\n",
    "\n",
    "R = np.array([-10, -10, 0])\n",
    "R = R.reshape((3,1))\n",
    "\n",
    "gamma = 0.999999999\n",
    "\n",
    "P = np.array([[0, 1, 0],\n",
    "              [0, 0, 1],\n",
    "              [0, 0, 1]])\n",
    "\n",
    "\n",
    "v = np.linalg.inv(np.eye(3,3) - gamma * P).dot(R)\n",
    "\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac1386b-a57b-4168-b79c-449e3cea61b6",
   "metadata": {},
   "source": [
    "## Markov Decision Process\n",
    "\n",
    "The Markov decision process is the same as the markov reward process but with the introduction of actions into the picture. You have the following elements in a markov reward process:\n",
    "1. $S$ is a finite set of states\n",
    "2. $A$ is a finite set of actions\n",
    "3. $P$ is a state transition probability matrix $P_{ss'}^a = P[S_{t+1} = s' | S_t = s, A_t = a]$\n",
    "4. $R$ is a reward function, $R_s^a = E[R_{t+1} | S_t = s, A_t = a]$\n",
    "5. $\\gamma$ is a discount factor $\\gamma \\in [0,1]$\n",
    "\n",
    "\n",
    "### Policy\n",
    "In a markov decision process we're trying to determine the policy.\n",
    "\n",
    "$$\n",
    "\\pi (a|s) = P[A_t = a | S_t = s]\n",
    "$$\n",
    "\n",
    "\n",
    "### The Value Function and Action Value functions\n",
    "\n",
    "![mdp_value_function](../../images/mdp_value_function.png)\n",
    "![mdp_action_value_function](../../images/mdp_action_value_function.png)\n",
    "![mdp_optimal_policy](../../images/mdp_optimal_policy.png)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
