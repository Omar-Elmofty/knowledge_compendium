{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76dcb020",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Model Free Control\n",
    "\n",
    "See the lecture [video](https://www.youtube.com/watch?v=0g4j2k_Ggc4&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ).\n",
    "See the lecture [notes](https://davidstarsilver.wordpress.com/wp-content/uploads/2025/04/lecture-5-model-free-control-.pdf)\n",
    "\n",
    "\n",
    "On-policy learning: **Learn on the job**. Learn about policy $\\pi$ from experience sampled from $\\pi$.\n",
    "\n",
    "Off-policy learning: **Look over someone's shoulder**, learn about policy $\\pi$  from experience sampled from $\\mu$\n",
    "\n",
    "\n",
    "We could use the generalized policy iteration framework with both Monte-Carlo, and TD to form a model-free method for learning the policy.\n",
    "\n",
    "## Epsilon-Greed Exploration\n",
    "\n",
    "As opposed to dynamic programming where we had a model of the system, here we don't have a model of the system, so we can't use the value function $V$, we have to use the action-value function $Q$ since it abstracts away the model.\n",
    "\n",
    "The other issue is that with dynamic programming, we did full sweeps of the state space. Hence, it was ok to be fully greedy when improving the policy since you're not prone to getting stuck in local minima. However, with sampling approaches like Monte-Carlo or TD, if you take the fully greedy solution after sampling a small section of the state space, you may get stuck with a suboptimal policy since you don't know if there is a better solution out there since you didn't explore the entire state space.\n",
    "The solution to that is to do $\\epsilon$-greedy exploration, where epsilon amount of times we will take a random action (exploration) and (1 - epsilon) times we will take the greedy action (exploitation).\n",
    "\n",
    "\n",
    "We could then use TD \n",
    "\n",
    "\n",
    "## Monte-Carlo vs. TD\n",
    "\n",
    "TD has advantages over MC:\n",
    "- Lower-variance (but high bias as opposed to MC)\n",
    "- Online (don't have to wait till the end of the episode like MC).\n",
    "- Incomplete sequences (the problem has to terminate).\n",
    "- Fits well with problems that are Markov (MC fits will with non-markov problems.\n",
    "\n",
    "\n",
    "\n",
    "## SARSA Algorithm (On-Policy)\n",
    "\n",
    "The SARSA algorithm uses TD with epsilon greedy exploration to find the optimal policy using the generalized policy iteration framework.\n",
    "\n",
    "Let's program it out:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "cdc3d9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def greedy(action_values):\n",
    "    return max(action_values, key=action_values.get)\n",
    "\n",
    "def e_greedy(action_values, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        # Return a random action\n",
    "        random_action = random.choice(list(action_values.keys()))\n",
    "        return random_action\n",
    "    else:\n",
    "        # Return action with the max Q-value\n",
    "        return greedy(action_values)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Pefrom the SARSA algorithm.\n",
    "Inputs:\n",
    "- Q: ns x na q-value lookup table.\n",
    "- R: ns reward lookup table\n",
    "- s_init: Initial s at the beginning of an episode.\n",
    "- s_terminal_set: Set containing terminal states if we ever end at we will stop.\n",
    "- epsilon: The epsilon for the e-greedy policy.\n",
    "- alpha: The learning rate.\n",
    "- gamma: The discount factor.\n",
    "- simulate: a function that simulates taking an action\n",
    "- n_episodes: Number of episodes to run.\n",
    "- max_iterations: the max_iterations to run.\n",
    "\"\"\"\n",
    "def sarsa(Q, R, s_init, s_terminal_set, epsilon, alpha, gamma, simulate, n_episodes, max_iterations=1000):\n",
    "    for _ in range(n_episodes):\n",
    "        s = s_init\n",
    "        a = e_greedy(Q[s], epsilon)\n",
    "        epsilon -= 1 / n_episodes * epsilon\n",
    "        iteration = 0\n",
    "        while (iteration < max_iterations):\n",
    "            if s in s_terminal_set:\n",
    "                Q[s][a] = R[s]\n",
    "                break\n",
    "            r = R[s]\n",
    "            s_prime = simulate(s, a)\n",
    "            a_prime = e_greedy(Q[s_prime], epsilon)    \n",
    "            Q[s][a] = Q[s][a] + alpha * (r + gamma * Q[s_prime][a_prime] - Q[s][a])\n",
    "            s = s_prime\n",
    "            a = a_prime\n",
    "            iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "e7d20635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[['d' 'd' 'd' 'd' 'd']\n",
      " ['d' 'd' 'd' 'd' 'd']\n",
      " ['d' 'd' 'd' 'd' 'd']\n",
      " ['d' 'd' 'd' 'd' 'd']\n",
      " ['u' 'u' 'u' 'u' 'u']]\n",
      "R:\n",
      "[[   10 -1000     0     0     0]\n",
      " [    0 -1000     0     0     0]\n",
      " [    0 -1000     0     0     0]\n",
      " [    0 -1000     0     0     0]\n",
      " [    0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Perform SARSA over a grid\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def simulate(s, a):\n",
    "    return (s[0] + a[0], s[1] + a[1])\n",
    "\n",
    "N = 5 # grid size\n",
    "\n",
    "# Initialize Q\n",
    "Q = defaultdict(dict)\n",
    "for x in range(N):\n",
    "    for y in range(N):\n",
    "        for ax, ay in [(1, 0), (-1, 0), (0, 1), (0, -1)]:\n",
    "            if (x + ax >= N) or (ax < 0 and x == 0):\n",
    "                continue\n",
    "            if (y + ay >= N) or (ay < 0 and y == 0):\n",
    "                continue\n",
    "            Q[(x, y)][(ax, ay)] = 0\n",
    "\n",
    "# Plot greedy Q\n",
    "def print_greedy_Q(Q, N):\n",
    "    array = np.array([[0 for i in range(N)]for i in range(N)])\n",
    "    a_array = np.array([[\"a\" for i in range(N)]for i in range(N)])\n",
    "    \n",
    "    for s, action_values in Q.items():\n",
    "        greedy_a = greedy(action_values)\n",
    "        array[s[0]][s[1]] = action_values[greedy_a]\n",
    "        if greedy_a == (1, 0):\n",
    "            a_array[s[0]][s[1]] = 'd'\n",
    "        elif greedy_a == (-1, 0):\n",
    "            a_array[s[0]][s[1]] = 'u'\n",
    "        elif greedy_a == (0, 1):\n",
    "            a_array[s[0]][s[1]] = 'r'\n",
    "        else:\n",
    "            a_array[s[0]][s[1]] = 'l'\n",
    "            \n",
    "        \n",
    "    print(array)\n",
    "    print(a_array)\n",
    "\n",
    "print(\"Q:\")\n",
    "print_greedy_Q(Q, N)\n",
    "\n",
    "# Initialize R\n",
    "R = {}\n",
    "for x in range(N):\n",
    "    for y in range(N):\n",
    "        R[(x, y)] = 0\n",
    "R[(0,0)] = 10\n",
    "\n",
    "# Create an obstacle for R\n",
    "R[(0, 1)] = -1000\n",
    "R[(1, 1)] = -1000\n",
    "R[(2, 1)] = -1000\n",
    "R[(3, 1)] = -1000\n",
    "\n",
    "def print_R(R, N):\n",
    "    array = np.array([[0 for i in range(N)]for i in range(N)])\n",
    "    for s, r in R.items():\n",
    "        array[s[0]][s[1]] = r\n",
    "    print(array)\n",
    "\n",
    "print(\"R:\")\n",
    "print_R(R, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "ee43edf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   10 -1000  -311  -312  -212]\n",
      " [    9 -1000  -297  -212  -182]\n",
      " [    9 -1000  -192  -179  -159]\n",
      " [    9 -1000  -122  -149  -143]\n",
      " [  -45   -30   -15   -35   -51]]\n",
      "[['d' 'd' 'r' 'r' 'd']\n",
      " ['u' 'd' 'r' 'd' 'd']\n",
      " ['u' 'd' 'r' 'r' 'd']\n",
      " ['u' 'd' 'd' 'd' 'd']\n",
      " ['u' 'l' 'l' 'l' 'l']]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the rest of params\n",
    "s_init = (4, 4)\n",
    "s_terminal_set = set([(0, 0), (0, 1), (1, 1), (2, 1), (3, 1)])\n",
    "epsilon = 0.1\n",
    "alpha = 0.1\n",
    "gamma = 1.0\n",
    "n_episodes = 10000\n",
    "max_iterations = 1e6\n",
    "sarsa(Q, R, s_init, s_terminal_set, epsilon, alpha, gamma, simulate, n_episodes, max_iterations)\n",
    "print_greedy_Q(Q, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0b7ec2",
   "metadata": {},
   "source": [
    "### Observations about SARSA\n",
    "\n",
    "It's great but takes a long time to converge, as it's on policy.\n",
    "\n",
    "## Q Learning\n",
    "\n",
    "Q Learning is an off policy learning approach. In the off policy approach, the robot follows a policy while improving another policy, in the q learning approach. The robot does the following:\n",
    "1. Follows epsilon greedy policy\n",
    "2. Improves the fully greedy policy\n",
    "\n",
    "Let's go through a code example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "4ec43c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pefrom the Q-learning algorithm.\n",
    "Inputs:\n",
    "- Q: ns x na q-value lookup table.\n",
    "- R: ns reward lookup table\n",
    "- s_init: Initial s at the beginning of an episode.\n",
    "- s_terminal_set: Set containing terminal states if we ever end at we will stop.\n",
    "- epsilon: The epsilon for the e-greedy policy.\n",
    "- alpha: The learning rate.\n",
    "- gamma: The discount factor.\n",
    "- simulate: a function that simulates taking an action\n",
    "- n_episodes: Number of episodes to run.\n",
    "- max_iterations: the max_iterations to run.\n",
    "\"\"\"\n",
    "def q_learning(Q, R, s_init, s_terminal_set, epsilon, alpha, gamma, simulate, n_episodes, max_iterations=1000):\n",
    "    for _ in range(n_episodes):\n",
    "        s = s_init\n",
    "        iteration = 0\n",
    "        while (iteration < max_iterations):\n",
    "            a = e_greedy(Q[s], epsilon)\n",
    "            r = R[s]\n",
    "            if s in s_terminal_set:\n",
    "                Q[s][a] = r\n",
    "                break\n",
    "            s_prime = simulate(s, a)\n",
    "            a_max = greedy(Q[s_prime])\n",
    "            Q[s][a] = Q[s][a] + alpha * (r + gamma * Q[s_prime][a_max] - Q[s][a])\n",
    "            s = s_prime\n",
    "            iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "2a6800ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   10  -564     2     0     0]\n",
      " [    9 -1000     9     1     1]\n",
      " [    9 -1000     9     9     9]\n",
      " [    9 -1000     9     9     9]\n",
      " [    9     9     9     9     9]]\n",
      "[['d' 'l' 'd' 'd' 'd']\n",
      " ['u' 'd' 'd' 'd' 'd']\n",
      " ['u' 'd' 'd' 'd' 'd']\n",
      " ['u' 'd' 'd' 'd' 'd']\n",
      " ['u' 'l' 'l' 'l' 'l']]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the rest of params\n",
    "s_init = (4, 4)\n",
    "s_terminal_set = set([(0, 0), (0, 1), (1, 1), (2, 1), (3, 1)])\n",
    "epsilon = 0.1\n",
    "alpha = 0.1\n",
    "gamma = 1.0\n",
    "n_episodes = 10000\n",
    "max_iterations = 1e6\n",
    "q_learning(Q, R, s_init, s_terminal_set, epsilon, alpha, gamma, simulate, n_episodes, max_iterations)\n",
    "print_greedy_Q(Q, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2b2a1e",
   "metadata": {},
   "source": [
    "### Observations on Q-learning\n",
    "\n",
    "Much more efficient than SARSA, it finds the solutions in much fewer iterations. Mainly because it is exploring, but at the same time updating Q function with the most greedy option. It's a good balance between the 2 options."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
